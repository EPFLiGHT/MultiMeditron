base_llm: meta-llama/Llama-3.1-8B-Instruct
base_model: 
attachment_token: <|reserved_special_token_0|>
tokenizer_type: llama
token_size: 4096

modalities:
  # - model_type: meditron_perception_encoder
  #   config:
  #     pe_name: PE-Lang-L14-448
  #- model_type: meditron_clip
   # config:
    #   clip_name: openai/clip-vit-large-patch14
  - model_type: moe_meditron_clip
    config:
       clip_name: openai/clip-vit-large-patch14

training_mode: ALIGNMENT

datasets:
  - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/BUSI
  - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/COVID_US
  - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/ct2
  - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/DDTI
  - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/iu_xray
  - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/llava_instruct
  - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/llava_pretrain
  # - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/medtrinity_pretrain_1
  # - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/medtrinity_pretrain_2
  - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/pixmo_anything
  - packed_path: /capstor/store/cscs/swissai/a127/meditron/multimediset/cleaned/image/PMC_VQA
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/BUSI
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/COVID_US
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/DDTI
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/PMC_VQA
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/ct2
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/iu_xray
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/llava_instruct
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/llava_pretrain
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/medtrinity_pretrain_1
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/medtrinity_pretrain_2
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/pixmo_anything
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/mammoth_ov_filtered
  # - packed_path: /mloscratch/users/merged_mediset/modality_merged/image_cleaned/pixmo_qa 

training_args:
  output_dir: models/MultiMeditron-8B-Alignment-Image-PE/July8
  dataloader_num_workers: 16  # > 0 not supported for IterableDataset, cf. https://github.com/huggingface/datasets/issues/5984
  dataloader_prefetch_factor: 4
  remove_unused_columns: false
  ddp_find_unused_parameters: false # Test to reduce memory
  learning_rate: 1.0e-4
  bf16: true
  per_device_train_batch_size: 1  # note that training_args.n_gpu and training_args.train_batch_size show faulty values
                                  # with deepspeed -> use deepspeed_plugin instead (besides training_args.distributed_state.num_processes == WORLD_SIZE)
  gradient_accumulation_steps: 16
  num_train_epochs: 2
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: true
  save_strategy: epoch
  max_grad_norm: 1.0
  run_name: MultiMeditron-8B-Alignment-Image-PE-Norm     # Set the name for the run
  deepspeed: ./config/deepspeed.json
  accelerator_config:
    dispatch_batches: false
  lr_scheduler_type: "cosine_with_min_lr"
  lr_scheduler_kwargs:
    min_lr: 3.0e-5
  report_to: wandb
  logging_steps: 1
  weight_decay: 0.01

