from tqdm import tqdm
from typing import Dict, List, Tuple, Union
from transformers import VisionTextDualEncoderModel, AutoImageProcessor, AutoTokenizer
from torchvision.io import ImageReadMode, read_image
from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize
from torchvision.transforms.functional import InterpolationMode

import os
import torch

DataRow = Dict[str, Union[str, List[Dict[str, str]]]]

image_processor = AutoImageProcessor.from_pretrained("openai/clip-vit-base-patch32")

def load_model(model_path: str) -> VisionTextDualEncoderModel:
    """
    Load the VisionTextDualEncoderModel from the provided path.

    Args:
      model_path (str): the path to the model
    
    Returns the corresponding VisionTextDualEncoderModel instance.
    """

    return VisionTextDualEncoderModel.from_pretrained(model_path)

# Preprocessing pipeline for images
class Transform(torch.nn.Module):
    def __init__(self, image_size, mean, std):
        super().__init__()
        self.transforms = torch.nn.Sequential(
            Resize([image_size], interpolation=InterpolationMode.BICUBIC, antialias=True),
            CenterCrop(image_size),
            ConvertImageDtype(torch.float),
            Normalize(mean, std),
        )

    def forward(self, x) -> torch.Tensor:
        "`x` should be an instance of `PIL.Image.Image`"
        with torch.no_grad():
            x = self.transforms(x)
        return x
            
image_transformations = Transform(
    224, image_processor.image_mean, image_processor.image_std
)
image_transformations = torch.jit.script(image_transformations)
img_transform = lambda img_path: image_transformations(read_image(img_path, mode=ImageReadMode.RGB))

def encode_img(model: VisionTextDualEncoderModel, img_path: str):
    """
    Encode a single image with a given model.

    Args:
      model: the image encoder.
      img_path: the absolute or relative path to the image that has to be encoded.

    Returns:
      The normalized embedding of the image generated by the model.
    """

    pixel_values = torch.stack([img_transform(img_path)])

    with torch.no_grad():
        image_embed = model.get_image_features(pixel_values)

    image_embed /= image_embed.norm(dim=-1, keepdim=True)
    return image_embed[0]

def preprocess_dataset(data: List[Dict], dataset_path: str) -> List[torch.Tensor]:
    """
    Preprocess a whole dataset.

    Args:
      data: list of the examples in the dataset that have to be processed.
      dataset_path: path to the root of the dataset (since image paths in the dataset are relative to it).

    Returns:
      A list containing the preprocessed images of the provided list of examples.
    """

    def adapt_line(line: DataRow):
        img_path = os.path.join(dataset_path, line["modalities"][0]["value"])
        return img_transform(img_path)

    #return list(map(adapt_line, chosen))
    return [adapt_line(line) for line in tqdm(data)]

def make(model: VisionTextDualEncoderModel, dataset: Union[torch.Tensor, List[torch.Tensor]]) -> torch.Tensor:
    """
    Make image embeddings for a dataset, from a given model.

    Args:
      model: the image encoder.
      dataset: a batch tensor or a list of tensors of preprocessed images to encode with the provided model.
    
    Returns:
      A tensor of the full encoded batch.
    """

    if not isinstance(dataset, list):
        dataset = [dataset]

    pixel_values = torch.stack(dataset)

    # Forward pass
    with torch.no_grad():
        image_embeds = model.get_image_features(pixel_values)

    # Normalization
    image_embeds /= image_embeds.norm(dim=-1, keepdim=True)

    return image_embeds